{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1pl86gxLTKgRn_HlmMEVtjpJL9dsvoSCl",
      "authorship_tag": "ABX9TyPQPoz4DzAorLc9hKVNnJkY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sourabh92133/medicine_rag_system/blob/main/Medical_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2B-J-kjl62F0"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "!ls\"/content/drive/MyDrive/Medicine_database.csv\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n",
        "!pip install langchain langchain_chroma chromadb scikit-learn plotly\n",
        "!pip install langchain-groq\n",
        "!pip install -U langchain_community"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fNXD_Fpi7V3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_text_splitters"
      ],
      "metadata": {
        "collapsed": true,
        "id": "DIAEEwgf_wk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import CSVLoader\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "from langchain_chroma import Chroma\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.manifold import TSNE"
      ],
      "metadata": {
        "id": "BGDKq1qm72-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "hu5t_Qq18th2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader=CSVLoader(file_path=\"/content/drive/MyDrive/Medicine_database.csv\",source_column=\"product_manufactured\",encoding=\"ISO-8859-1\")\n",
        "document=[]\n",
        "lst_doc=loader.load()\n",
        "print(lst_doc[0])\n",
        "for doc in lst_doc:\n",
        "  doc.metadata[\"manufacture\"]=doc.metadata.get(\"source\")\n",
        "  if \"source\" in doc.metadata:\n",
        "    del doc.metadata[\"source\"]\n",
        "  document.append(doc)"
      ],
      "metadata": {
        "id": "8c-PSfmWAjVd",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(document))\n",
        "print(type(document[0]))\n",
        "document[0]"
      ],
      "metadata": {
        "id": "Z7H5MA1YRY2D",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chunking\n",
        "textsplitter=CharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
        "chunk=textsplitter.split_documents(document)"
      ],
      "metadata": {
        "id": "KXqB0NOWDP7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(chunk))\n",
        "type(chunk)\n"
      ],
      "metadata": {
        "id": "efjdR4mqTh2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunk[0]"
      ],
      "metadata": {
        "id": "ybJGoVf5TjHW",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating embedding model\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "embedding=HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "P0THhhPNTrIS",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db_name=\"vector_database\"\n",
        "if os.path.exists(db_name):\n",
        "  Chroma(persist_directory=db_name,embedding_function=embedding).delete_collection()  # to delete vector store if exists"
      ],
      "metadata": {
        "id": "9-5dX4GNUViE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store=Chroma.from_documents(embedding=embedding,documents=chunk,persist_directory=db_name)    # this will create a vector store"
      ],
      "metadata": {
        "id": "hm1XoZAmWYYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualization of 2 vector\n",
        "vector=vector_store._collection\n",
        "vec=vector.get(limit=2,include=[\"embeddings\",\"documents\",\"metadatas\"])\n",
        "print(vec)\n",
        "print(len(vec[\"embeddings\"][0]))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "VKG0MrsFWvQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectors=vector_store._collection\n",
        "vector_all=vectors.get(include=[\"embeddings\",\"documents\",\"metadatas\"])  # this will show dict of all vectors\n",
        "print(type(vector_all))\n",
        "vector_np=np.array(vector_all[\"embeddings\"])        # we converted vectors_all dict into numpy array because mathematics tools can be apply only on numpy array like tsne\n",
        "manufacture= [metadata[\"manufacture\"] for metadata in vector_all[\"metadatas\"]]\n",
        "print(manufacture)"
      ],
      "metadata": {
        "id": "3wZkAGniX1VT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let us create a FAISS database\n",
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ppJJznk7pory"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS      # in faiss we only store faiss id corresponding to which vector is stored\n",
        "vector_store_FAISS=FAISS.from_documents(documents=chunk,embedding=embedding)"
      ],
      "metadata": {
        "id": "KBEF3Gx5oyH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let us see FAISS vectorr\n",
        "number=vector_store_FAISS.index.ntotal     # total number of vectors\n",
        "dimension=vector_store_FAISS.index.d       # dimension of vectors\n",
        "print(vector_store_FAISS.index.reconstruct(0))     # vector at index 0\n",
        "print(number,dimension)\n",
        "vector_of_faiss=[]\n",
        "documents=[]\n",
        "for i in range(number):\n",
        "  doc_id=vector_store_FAISS.index_to_docstore_id[i]      # to print doc id of document stored corrresponding to Faiss id\n",
        "  document=vector_store_FAISS.docstore.search(doc_id)\n",
        "  documents.append(document)\n",
        "  vector_of_faiss.append(vector_store_FAISS.index.reconstruct(i))\n",
        "# print(documents[0])\n",
        "print(type(vector_store_FAISS))\n",
        "vectors_Faiss_np=np.array(vector_of_faiss)\n",
        "print(type(vectors_Faiss_np))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "TgVdRo3kp5pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualization of vectors in 2d of chroma db\n",
        "tsne=TSNE(n_components=2,random_state=42)\n",
        "reduced_vec_dimension=tsne.fit_transform(vector_np)\n",
        "fig=go.Figure(data=go.Scatter(\n",
        "                              x=reduced_vec_dimension[:,0],\n",
        "                              y=reduced_vec_dimension[:,1],\n",
        "                              mode=\"markers\",\n",
        "                              marker=dict(size=10,opacity=1,color=\"red\"),\n",
        "                              text=[f\"manufacture_by: {t}, Info:{d[:100]}...\" for t,d in zip(manufacture,vector_all[\"documents\"])],\n",
        "                              hoverinfo=\"text\"\n",
        "))\n",
        "fig.update_layout(\n",
        "    title=\"Chroma_db_vectors2d\",\n",
        "    xaxis_title=\"X\",\n",
        "    yaxis_title=\"Y\",\n",
        "    width=800,\n",
        "    height=800,\n",
        "    margin=dict(l=0,r=0,b=0,t=50)     # padding (in pixels ) between edges and graph area\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "OyY81xGlZ-Nc",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to look in 3d\n",
        "tsne_3d=TSNE(n_components=3,random_state=42)\n",
        "vec_dimension3d=tsne_3d.fit_transform(vector_np)\n",
        "fig_new=go.Figure(data=go.Scatter3d(\n",
        "    x=vec_dimension3d[:,0],\n",
        "    y=vec_dimension3d[:,1],\n",
        "    z=vec_dimension3d[:,2],\n",
        "    mode=\"markers\",\n",
        "    marker=dict(size=10,opacity=1,color=\"red\"),\n",
        "    text=[f\"manufactured_by: {t} , info:{d[:100]}...\" for t,d in zip(manufacture,vector_all[\"documents\"])],\n",
        "    hoverinfo=\"text\"\n",
        "\n",
        "))\n",
        "fig_new.update_layout(\n",
        "    title=\"chroma_db_vectors in 3d\",\n",
        "    scene=dict(xaxis_title=\"X\",yaxis_title=\"Y\",zaxis_title=\"Z\"),\n",
        "    width=800,\n",
        "    height=800,\n",
        "    margin=dict(l=0,r=0,b=0,t=50)\n",
        ")\n",
        "fig_new.show()"
      ],
      "metadata": {
        "id": "JPyvSsSCeJKx",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualizion of vector in 2d of faiss\n",
        "tsne=TSNE(n_components=2,random_state=42)\n",
        "vec_dimension2d=tsne.fit_transform(vectors_Faiss_np)\n",
        "fig=go.Figure(data=go.Scatter(\n",
        "    x=reduced_vec_dimension[:,0],\n",
        "    y=reduced_vec_dimension[:,1],\n",
        "    mode=\"markers\",\n",
        "    marker=dict(size=10,opacity=1,color=\"red\"),\n",
        "    text=[f\"manufactured_by: {t} , info:{d[:100]}...\" for t, d in zip(manufacture,vector_all[\"documents\"])],\n",
        "    hoverinfo=\"text\"\n",
        "))\n",
        "fig.update_layout(\n",
        "    title=\"FAISS_vectors_visualization in 2d\",\n",
        "    xaxis_title=\"X\",\n",
        "    yaxis_title=\"Y\",\n",
        "    width=800,\n",
        "    height=800,\n",
        "    margin=dict(l=0,r=0,b=0,t=50)\n",
        ")\n",
        "fig.show()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "J-974JcktAWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "print(langchain.__version__)\n"
      ],
      "metadata": {
        "id": "4-ukccnohJq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-classic"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KMWpzMiQiHiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_classic.memory import ConversationBufferMemory\n",
        "from langchain_classic.chains import ConversationalRetrievalChain\n",
        "from langchain_groq import ChatGroq\n"
      ],
      "metadata": {
        "id": "N0jw4kE0gCRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "os.environ[\"GROQ_API_KEY\"]=userdata.get(\"GROQ_API_KEY\")\n"
      ],
      "metadata": {
        "id": "RK1Lu8OPnDp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm =ChatGroq(model_name=\"openai/gpt-oss-120b\",temperature=0.4)\n",
        "# setting up memory to store chat\n",
        "memory=ConversationBufferMemory(memory_key=\"chat_history\",return_messages=True)      # memory_key is simply the variable name under which conversation history is stored and passed into the LLM prompt.\n",
        "# setting up retriever that will retrieve related info and send to llm\n",
        "retriever=vector_store.as_retriever()\n",
        "#setting up chain\n",
        "conversation_chain=ConversationalRetrievalChain.from_llm(llm=llm,retriever=retriever,memory=memory)\n"
      ],
      "metadata": {
        "id": "NM82ICbg2VRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BATCHING SUPPORT FOR MULTIPLE QUERIES\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import time, random\n",
        "\n",
        "# Create a no-memory chain for safe parallel batch queries\n",
        "conversation_chain_no_mem = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    memory=None\n",
        ")\n",
        "\n",
        "def call_chain_single_with_retry(chain, question, retries=3, base_delay=1.0):\n",
        "    \"\"\"Call chain with retry & exponential backoff.\"\"\"\n",
        "    def _call():\n",
        "        return chain({\"question\": question})\n",
        "\n",
        "    for attempt in range(1, retries+1):\n",
        "        try:\n",
        "            return _call()\n",
        "        except Exception as e:\n",
        "            if attempt == retries:\n",
        "                raise\n",
        "            sleep = base_delay * (2 ** (attempt-1)) + random.random()*0.2\n",
        "            time.sleep(sleep)\n",
        "\n",
        "def batch_queries(queries, chain, max_workers=3):\n",
        "    \"\"\"Run multiple queries in parallel.\"\"\"\n",
        "    results = [None]*len(queries)\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
        "        futures = {ex.submit(call_chain_single_with_retry, chain, q): i\n",
        "                   for i,q in enumerate(queries)}\n",
        "\n",
        "        for fut in as_completed(futures):\n",
        "            i = futures[fut]\n",
        "            try:\n",
        "                out = fut.result()\n",
        "                ans = out.get(\"answer\") or out.get(\"output_text\") or str(out)\n",
        "            except Exception as e:\n",
        "                ans = f\"ERROR: {e}\"\n",
        "            results[i] = {\"question\": queries[i], \"answer\": ans}\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "KuPyael_x8Yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question=\"medicine for cold?\"\n",
        "result=conversation_chain.invoke({\"question\":question})\n",
        "print(result[\"answer\"])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "x9rX_XFinWBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradio_function(message, history):\n",
        "\n",
        "    # Split by lines → detect multiquery batch\n",
        "    queries = [q.strip() for q in message.split(\"\\n\") if q.strip()]\n",
        "\n",
        "    # If more than 1 question → BATCH MODE\n",
        "    if len(queries) > 1:\n",
        "        results = batch_queries(queries, conversation_chain_no_mem, max_workers=3)\n",
        "\n",
        "        final_output = \"\"\n",
        "        for item in results:\n",
        "            final_output += f\"Q: {item['question']}\\nA: {item['answer']}\\n\"\n",
        "            final_output += \"-\"*40 + \"\\n\"\n",
        "\n",
        "        return final_output\n",
        "\n",
        "    # Single question → use conversational chain with memory\n",
        "    out = conversation_chain({\"question\": queries[0]})\n",
        "    return out.get(\"answer\") or str(out)"
      ],
      "metadata": {
        "id": "eAwTjQfYi0I_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gr.ChatInterface(gradio_function).launch(share=True)"
      ],
      "metadata": {
        "id": "aR1NIG0pmHo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NKgkwY0oyNYT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
